{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from prep_data_modular import prep_data\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer, get_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.telegram import tqdm as telegram_tqdm\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "from utility import *\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from itertools import combinations, compress\n",
    "from scipy.stats import ttest_ind \n",
    "from statsmodels.stats.multitest import multipletests\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPETITION = 10 if is_server() else 1 # auto sets rep to 10 when executing on server\n",
    "CROSS_VAL = 5\n",
    "RANDOM_STATE = 42\n",
    "EXEC_MODE = 'load'\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# default \n",
    "N_ESTIMATOR = 1000\n",
    "MAX_DEPTH = 5\n",
    "CRITERION = 'entropy'\n",
    "MIN_SAMPLES_LEAF = 5 \n",
    "MAX_FEATURES = 'sqrt' # 0.3\n",
    "MAX_SAMPLES = 0.75 # None # = all\n",
    "CLASS_WEIGHT = 'balanced'\n",
    "\n",
    "metrics = [\"test_roc_auc\", \"test_f1\", \"test_mcc\", \"test_pr_auc\"] #, \"test_precision\"]\n",
    "# groups = [\"study_accession\", \"country\"]\n",
    "groups = [\"cohort_name\"]\n",
    "\n",
    "# # thomas et al\n",
    "# N_ESTIMATOR = 1000\n",
    "# MAX_DEPTH = None\n",
    "# CRITERION = 'entropy'\n",
    "# MIN_SAMPLES_LEAF = 5\n",
    "# MAX_FEATURES = 0.3\n",
    "# MAX_SAMPLES = None # = all\n",
    "# CLASS_WEIGHT = None\n",
    "\n",
    "N_JOBS=-1\n",
    "MAX_CONCURRENT = 10\n",
    "# basic preprocessing and concatination of raw input files\n",
    "REDO_DATA_FORMATING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REDO_DATA_FORMATING:\n",
    "    prep_data(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets_file = \"./secrets.json\"\n",
    "\n",
    "try:\n",
    "    with open(secrets_file) as f:\n",
    "        secrets = json.load(f)\n",
    "        telegram_token = secrets['telegram_token']\n",
    "        telegram_chatid = secrets['telegram_chatid']\n",
    "except:\n",
    "    print(\"Secrets file not found. No Telegram notifications will be sent.\")\n",
    "    telegram_token = \"\"\n",
    "    telegram_chatid = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = '/media/niklas/T7/data/FeMAI/source_data/'\n",
    "data_dir = './prep_data'\n",
    "data_dir = Path(data_dir)\n",
    "\n",
    "taxa = 'taxa.csv'\n",
    "func = 'func.csv'\n",
    "taxa_func = 'taxa_func.csv'\n",
    "\n",
    "taxa_meta4 = 'taxa_meta4.csv'\n",
    "\n",
    "taxa_counts_0 = 'taxa_counts_0.csv'\n",
    "func_counts_0 = 'func_counts_0.csv'\n",
    "taxa_func_counts_0 = 'taxa_func_counts_0.csv'\n",
    "\n",
    "taxa_counts_10 = 'taxa_counts_10.csv'\n",
    "func_counts_10 = 'func_counts_10.csv'\n",
    "taxa_func_counts_10 = 'taxa_func_counts_10.csv'\n",
    "\n",
    "#anno_raw = 'anno_full_raw.csv' # remove to_exclude\n",
    "anno_all = 'anno_full_clean.csv' # + remove NAN in metadata\n",
    "\n",
    "default_out = './data'\n",
    "output_dir = default_out\n",
    "\n",
    "fig_dir = os.path.join(output_dir, 'figures')\n",
    "Path(fig_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "save_dir = os.path.join(output_dir, 'simulations')\n",
    "Path(save_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "table_dir = os.path.join(save_dir, 'tables')\n",
    "Path(table_dir).mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno = pd.read_csv(path.join(data_dir, anno_all), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno.groupby(by=['study_accession', 'country', 'cohort_name'])['MGS'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df_anno.loc[~df_anno['cohort_name'].isin(['Canada1', 'India1', 'India2'])].groupby(['study_accession', 'country','cohort_name']).agg(\n",
    "    num_samples=('sample_accession', 'count'),\n",
    "    CRC=('health_status', lambda x: (x == 1).sum()),\n",
    "    healthy=('health_status', lambda x: (x == 0).sum()),\n",
    "    female=('gender', lambda x: (x == 0).sum()),\n",
    "    male=('gender', lambda x: (x == 1).sum()),\n",
    "    age_mean=('age', 'mean'),\n",
    "    bmi_mean=('bmi', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Create final table by adding other columns manually\n",
    "final_table = pd.DataFrame({\n",
    "    'Alias': grouped_df['cohort_name'],\n",
    "    'Num. samples': grouped_df['num_samples'],\n",
    "    'CRC': grouped_df['CRC'],\n",
    "    '% CRC': np.round(grouped_df['CRC'] / grouped_df['num_samples'] * 100, 2),\n",
    "    'Adenoma': 0,\n",
    "    'Healthy': grouped_df['healthy'],\n",
    "    'Female': grouped_df['female'],\n",
    "    'Male': grouped_df['male'],\n",
    "    'NA': 0,  # Placeholder for NA column\n",
    "    'BMI Mean': np.round(grouped_df['bmi_mean'], 2),\n",
    "    'Age Mean': np.round(grouped_df['age_mean'], 2)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table.to_csv(os.path.join(table_dir, 'final_table.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age', 'bmi', 'gender', 'health_status', 'country']\n",
    "df_vars = df_anno[features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thomas et al 2019 paper :\n",
    "PRJNA447983\t\n",
    "https://www.nature.com/articles/s41591-019-0405-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "thomas_alias = {\n",
    "    'PRJDB4176': 'V_Cohort2',\n",
    "    'PRJEB10878': 'YuJ_2015',\n",
    "    'PRJEB12449': 'VogtmannE_2016',\n",
    "    'PRJEB27928': 'V_Cohort1',\n",
    "    'PRJEB6070':  'ZellerG_2014',\n",
    "    'PRJEB7774': 'FengQ_2015',\n",
    "    'PRJNA389927': 'HanniganGD_2018',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno[['thomas_alias', 'study_accession', 'country']].value_counts(sort=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### central sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    'accuracy': get_scorer('accuracy'), \n",
    "    'balanced_accuracy': get_scorer('balanced_accuracy'), \n",
    "    'roc_auc': get_scorer('roc_auc'),\n",
    "    'pr_auc': make_scorer(pr_auc_score),\n",
    "    'f1': get_scorer('f1'),\n",
    "    #'f2': make_scorer(fbeta_score, beta=2.0, zero_devision=np.nan),\n",
    "    'precision': make_scorer(precision_score, zero_devision = 0),\n",
    "    'recall': get_scorer('recall'),\n",
    "    #'sensitivity': make_scorer(sensitivity_score),\n",
    "    #'specificity': make_scorer(specificity_score),\n",
    "    'mcc': get_scorer('matthews_corrcoef'),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno_datasets_raw = {\n",
    "    'core': df_anno.loc[df_anno.study_accession.isin(['PRJNA429097', 'PRJEB6070', 'PRJEB27928', 'PRJEB10878', 'PRJNA731589'])].copy(),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experiment:\n",
    "    name: str\n",
    "    anno: str|Path\n",
    "    count: str|Path\n",
    "    datasets: dict[str, str|Path]\n",
    "    clean_name: str = ''\n",
    "    \n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.clean_name == '':\n",
    "            self.clean_name = clean(self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit rerun to only core\n",
    "experiments = [\n",
    "    Experiment(name= 'count_taxa_0',                anno= anno_all,        count= taxa_counts_0,       datasets= df_anno_datasets_raw),\n",
    "    Experiment(name= 'count_taxa_metaphlan4_0',     anno= anno_all,         count= taxa_meta4,          datasets= df_anno_datasets_raw),\n",
    "    \n",
    "    Experiment(name= 'count_taxa_func_0',           anno= anno_all,        count= taxa_func_counts_0,  datasets= df_anno_datasets_raw),\n",
    "    Experiment(name= 'count_func_0',                anno= anno_all,        count= func_counts_0,           datasets= df_anno_datasets_raw),\n",
    "    \n",
    "    \n",
    "    # Experiment(name= 'count_taxa_func_10',           anno= anno_all,        count= taxa_func_counts_10,  datasets= df_anno_datasets_raw),\n",
    "    # Experiment(name= 'count_taxa_10',               anno= anno_all,        count= taxa_counts_10,       datasets= df_anno_datasets_raw),\n",
    "    # Experiment(name= 'count_func_10',               anno= anno_all,        count= func_counts_10,           datasets= df_anno_datasets_raw),\n",
    "\n",
    "    # Experiment(name= 'taxa_func',               anno= anno_all,        count= taxa_func,        datasets= df_anno_datasets_raw),\n",
    "    # Experiment(name= 'taxa',                    anno= anno_all,        count= taxa,       datasets= df_anno_datasets_raw),\n",
    "    # Experiment(name= 'func',                    anno= anno_all,        count= func,       datasets= df_anno_datasets_raw),\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_server():\n",
    "    experiments = experiments[:1]\n",
    "    print(\"Only running minimal test \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(\n",
    "    anno_df: pd.DataFrame,\n",
    "    counts: pd.DataFrame,\n",
    "    simulation_type: str,\n",
    "    dataset_name: str,\n",
    "    groups: list,\n",
    "    clients: list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    label: str = \"health_status\",\n",
    "    REPETITION: int = 10,\n",
    "    N_ESTIMATOR: int = 100,\n",
    "    N_JOBS: int = -1,\n",
    "    CROSS_VAL: int = 5,\n",
    "    feature_importance: str = 'gini',\n",
    "    weighting: str = None,\n",
    "    data_dir: str = './',\n",
    "    scoring: dict = {'accuracy': get_scorer('accuracy')},\n",
    "    exec_mode: str = 'add',\n",
    "    filename: str = 'simulation_results.pkl',\n",
    "    save_path: str = './',\n",
    "    verbose: bool = False,\n",
    "    log_level: int = logging.ERROR,\n",
    "    log_file: str = \"execution_time.json\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Unified function to run different types of simulations.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    execution_log = {\n",
    "        \"simulation_type\": simulation_type,\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"start_time\": start_time,\n",
    "        \"start_time_str\" : datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"steps\": []\n",
    "    }\n",
    "    \n",
    "    def log_step(step_name, start, level=logging.INFO, **kwargs):\n",
    "        duration = time.time() - start\n",
    "        step_log = {\n",
    "            \"step_name\": step_name,\n",
    "            \"duration\": duration,\n",
    "            \"start_time\": start,\n",
    "            \"end_time\": start + duration\n",
    "        }\n",
    "        step_log.update(kwargs)  # Add any additional key-value pairs\n",
    "        execution_log[\"steps\"].append(step_log)\n",
    "        if log_level <= level:\n",
    "            print(f\"{step_name} took {str(timedelta(seconds=int(duration)))} seconds.\")\n",
    "\n",
    "    # Handle paths\n",
    "    save_path = Path(save_path) / filename\n",
    "    save_path_unfinished = save_path.with_suffix('.unfinished')\n",
    "    log_file = save_path.parent / log_file\n",
    "\n",
    "    # Check execution mode\n",
    "    if exec_mode not in ['redo', 'load', 'add']:\n",
    "        raise ValueError(f'Invalid exec_mode: {exec_mode}. Must be \"redo\" or \"load\".')\n",
    "\n",
    "    # Initialize results\n",
    "    res_out, f_imp_out = [], []\n",
    "\n",
    "    # Load previous results if necessary\n",
    "    if exec_mode == 'load':\n",
    "        if save_path.exists():\n",
    "            \n",
    "            if log_level <= logging.INFO:\n",
    "                print(f\"Loading previous results for {dataset_name}.\")\n",
    "            return \n",
    "        else:\n",
    "            print(f\"No previous results found for {dataset_name}. Starting from scratch.\")\n",
    "\n",
    "    # Ensure directory exists\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Common preparations\n",
    "    meta_features = ['health_status', 'gender', 'age', 'bmi'] + groups\n",
    "    to_drop = [label] + groups\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Merging annotation and count dataframes.\")\n",
    "\n",
    "    data = pd.merge(anno_df[meta_features], counts, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "    if data.empty:\n",
    "        raise ValueError(\"Merged data is empty after dropping NA values. Please check your input data.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Merged data contains {data.shape[0]} samples and {data.shape[1]} features.\")\n",
    "\n",
    "    # Mode-specific preparations\n",
    "    present_groups = [grp[0] for grp, _df in anno_df.groupby(by=groups) if len(_df[label].unique()) >= 2 and len(_df) >= CROSS_VAL*2]\n",
    "    datasets = {grp[0]: (grp_df.drop(columns=to_drop).values, grp_df[label].values) for grp, grp_df in data.groupby(groups) if grp[0] in present_groups}\n",
    "\n",
    "    pbar_desc = f'{simulation_type} - {dataset_name}'\n",
    "\n",
    "    if simulation_type == 'central':\n",
    "        pbar_total = REPETITION * CROSS_VAL\n",
    "    elif simulation_type == 'local':\n",
    "        pbar_total = len(present_groups) * REPETITION\n",
    "    else:  # 'combinations' or 'federated'\n",
    "        if not datasets:\n",
    "            raise ValueError(\"No valid groups found for the simulation. Please check your group criteria and input data.\")\n",
    "        pbar_total = sum(len(list(combinations(present_groups, n))) for n in clients) * REPETITION\n",
    "\n",
    "    log_step(f\"finish data prep\", start_time, level=logging.INFO)\n",
    "\n",
    "    # Track how many results have been written\n",
    "    results_written = 0\n",
    "\n",
    "    with tqdm(total=pbar_total, desc=pbar_desc) as pbar, open(save_path_unfinished, 'ab') as f:\n",
    "        for rep in range(REPETITION):\n",
    "            rep_start = time.time()\n",
    "            if simulation_type == 'central':\n",
    "                if verbose:\n",
    "                    print(f'Starting repetition {rep + 1}/{REPETITION}')\n",
    "\n",
    "                start = time.time()\n",
    "                X = data.drop(columns=to_drop).values\n",
    "                y = data[label].values\n",
    "\n",
    "                if X.size == 0 or y.size == 0:\n",
    "                    print(f\"Skipping repetition {rep + 1}/{REPETITION}: No data available after preprocessing.\")\n",
    "\n",
    "                rf = RandomForestClassifier(n_estimators=N_ESTIMATOR, max_depth=MAX_DEPTH, criterion=CRITERION, min_samples_leaf=MIN_SAMPLES_LEAF, max_features=MAX_FEATURES,\n",
    "                        class_weight=CLASS_WEIGHT,n_jobs=1, max_samples=MAX_SAMPLES)\n",
    "                kf = StratifiedKFold(n_splits=CROSS_VAL, shuffle=True, random_state=rep)\n",
    "\n",
    "                res, f_imp = custom_cross_validate(rf, X, y, features=data.drop(columns=to_drop).columns, cv=kf, scoring=scoring, feature_importance=feature_importance)\n",
    "\n",
    "                meta_inf = {\n",
    "                    \"rep\": [rep] * CROSS_VAL, \"train_dataset\": ['all'] * CROSS_VAL,\n",
    "                    \"val_dataset\": ['all'] * CROSS_VAL, \"analysis\": [dataset_name] * CROSS_VAL,\n",
    "                    \"cv\": list(range(CROSS_VAL)), 'n_clients': [1] * CROSS_VAL\n",
    "                }\n",
    "\n",
    "                res.update(meta_inf)\n",
    "                f_imp.update(meta_inf)\n",
    "                res_out.append(res)\n",
    "                f_imp_out.append(f_imp)\n",
    "\n",
    "                log_step(\"cross-validation\", start, level=logging.DEBUG)\n",
    "\n",
    "                pbar.update(CROSS_VAL)\n",
    "\n",
    "                # Write only the new results one by one in append mode\n",
    "                for new_res, new_f_imp in zip(res_out[results_written:], f_imp_out[results_written:]):\n",
    "                    pickle.dump((new_res, new_f_imp), f)\n",
    "\n",
    "                results_written += len(res_out[results_written:])\n",
    "\n",
    "            elif simulation_type == \"local\":\n",
    "                if verbose:\n",
    "                    print(f'Starting local cross-validation {rep + 1}/{REPETITION}')\n",
    "                for grp in present_groups:\n",
    "                    start = time.time()\n",
    "                    X, y = datasets[grp]\n",
    "                    if X.size == 0 or y.size == 0:\n",
    "                        print(f\"Skipping repetition {rep + 1}/{REPETITION} for group {grp}: No data available after preprocessing.\")\n",
    "                        continue\n",
    "\n",
    "                    rf = RandomForestClassifier(n_estimators=N_ESTIMATOR, max_depth=MAX_DEPTH, criterion=CRITERION, min_samples_leaf=MIN_SAMPLES_LEAF, max_features=MAX_FEATURES,\n",
    "                        class_weight=CLASS_WEIGHT,n_jobs=1, max_samples=MAX_SAMPLES)\n",
    "                    kf = StratifiedKFold(n_splits=CROSS_VAL, shuffle=True, random_state=rep)\n",
    "\n",
    "                    res, f_imp = custom_cross_validate(rf, X, y, features=data.drop(columns=to_drop).columns, cv=kf, scoring=scoring, feature_importance=feature_importance)\n",
    "\n",
    "                    meta_inf = {\n",
    "                        \"rep\": [rep] * CROSS_VAL, \"train_dataset\": [grp] * CROSS_VAL,\n",
    "                        \"val_dataset\": [grp] * CROSS_VAL, \"analysis\": [dataset_name] * CROSS_VAL,\n",
    "                        \"cv\": list(range(CROSS_VAL)), 'n_clients': [1] * CROSS_VAL\n",
    "                    }\n",
    "\n",
    "                    res.update(meta_inf)\n",
    "                    f_imp.update(meta_inf)\n",
    "                    res_out.append(res)\n",
    "                    f_imp_out.append(f_imp)\n",
    "\n",
    "                    log_step(\"cross-validation\", start, dataset=grp, level=logging.DEBUG)\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    # Write only the new results one by one in append mode\n",
    "                    for new_res, new_f_imp in zip(res_out[results_written:], f_imp_out[results_written:]):\n",
    "                        pickle.dump((new_res, new_f_imp), f)\n",
    "\n",
    "                    results_written += len(res_out[results_written:])\n",
    "            else:\n",
    "                for n_clients in clients:\n",
    "                    client_start = time.time()\n",
    "                    # if simulation_type == 'federated':\n",
    "                    #     start = time.time()\n",
    "                    #     estimators = {grp: RandomForestClassifier(n_estimators=N_ESTIMATOR//n_clients, max_depth=MAX_DEPTH, criterion=CRITERION, min_samples_leaf=MIN_SAMPLES_LEAF, max_features=MAX_FEATURES,\n",
    "                    #                     class_weight=CLASS_WEIGHT,n_jobs=1, max_samples=MAX_SAMPLES).fit(datasets[grp][0], datasets[grp][1]) for grp in datasets}\n",
    "                    #     log_step(\"training\", start, level=logging.DEBUG)\n",
    "                    for comb in combinations(present_groups, n_clients):\n",
    "                        test_data = [_ for _ in present_groups if _ not in comb]\n",
    "                            \n",
    "                        if simulation_type == 'combinations':\n",
    "                            train_data = [datasets[grp] for grp in comb if grp in datasets]\n",
    "                            if len(train_data) != len(comb):\n",
    "                                continue\n",
    "                            X, y = np.concatenate([dat[0] for dat in train_data]), np.concatenate([dat[1] for dat in train_data])\n",
    "\n",
    "                            if X.size == 0 or y.size == 0:\n",
    "                                print(f\"Skipping combination {comb}: No data available after preprocessing.\")\n",
    "                                continue\n",
    "\n",
    "                            start = time.time()\n",
    "                            rf = RandomForestClassifier(n_estimators=N_ESTIMATOR, max_depth=MAX_DEPTH, criterion=CRITERION, min_samples_leaf=MIN_SAMPLES_LEAF, max_features=MAX_FEATURES,\n",
    "                                        class_weight=CLASS_WEIGHT,n_jobs=1, max_samples=MAX_SAMPLES).fit(X, y)\n",
    "                            log_step(f\"training\", start, dataset=comb, level=logging.DEBUG)\n",
    "\n",
    "\n",
    "                            start = time.time()\n",
    "                            for grp in test_data:\n",
    "                                X_test, y_test = datasets[grp]\n",
    "                                res, f_imp = custom_comb_validate(rf, X, y, X_test, y_test, features=data.drop(columns=to_drop).columns.tolist(), scoring=scoring, feature_importance=feature_importance)\n",
    "                                \n",
    "                                meta_inf = {\n",
    "                                    \"rep\": [rep], \"train_dataset\": [', '.join(map(str, comb))] ,\n",
    "                                    \"val_dataset\": [grp], \"analysis\": [dataset_name],\n",
    "                                    \"cv\": [0], 'n_clients': [n_clients]\n",
    "                                }\n",
    "                                \n",
    "                                res.update(meta_inf)\n",
    "                                f_imp.update(meta_inf)\n",
    "                                res_out.append(res)\n",
    "                                f_imp_out.append(f_imp)\n",
    "                            log_step(f\"validation\", start, dataset=comb, level=logging.DEBUG)\n",
    "                            \n",
    "                        # elif simulation_type == 'federated':\n",
    "                            \n",
    "                        #     start = time.time()\n",
    "                        #     if weighting == 'mcc':\n",
    "                        #         # using method based on https://ieeexplore.ieee.org/document/9867984 \n",
    "                        #         # combinded MCC from each tree above threshold 0.2 as weight\n",
    "                        #         # removes about 80% of the trees -> worth a 2nd look\n",
    "                        #         weights = {}\n",
    "                        #         for grp in comb:\n",
    "                        #             weights[grp] = {i: [] for i in range(len(estimators[grp].estimators_))}\n",
    "                        #             for val_grp in comb:\n",
    "                        #                 if grp == val_grp:\n",
    "                        #                     continue\n",
    "                        #                 for i, tree in enumerate(estimators[grp].estimators_):\n",
    "                        #                     y_pred = tree.predict(datasets[val_grp][0])\n",
    "                        #                     conf = confusion_matrix(datasets[val_grp][1], y_pred)\n",
    "                        #                     weights[grp][i].append(conf)\n",
    "                        #             for i in weights[grp]:\n",
    "                        #                 summed_conf_matrix = np.sum(np.array(weights[grp][i]), axis=0)\n",
    "                        #                 if summed_conf_matrix.ndim == 1:\n",
    "                        #                     summed_conf_matrix = np.expand_dims(summed_conf_matrix, axis=0)\n",
    "                        #                 mcc = mcc_from_cm(summed_conf_matrix)\n",
    "                        #                 weights[grp][i] = mcc if mcc >= 0.2 else 0\n",
    "\n",
    "                        #         combined_estimators, combined_weights = [], []\n",
    "                        #         for grp in comb:\n",
    "                        #             combined_estimators.extend(estimators[grp].estimators_)\n",
    "                        #             combined_weights.extend(weights[grp].values())\n",
    "                        #         combined_rf = VotingClassifier(estimators=combined_estimators, weights=combined_weights, voting='soft')\n",
    "                        #     else:\n",
    "                        #         pass\n",
    "                            \n",
    "                        #     log_step(\"weighting\", start, dataset=comb, weighting=weighting, level=logging.DEBUG)\n",
    "                            \n",
    "                        #     start = time.time()\n",
    "                        #     for grp in test_data:\n",
    "                        #         X_test, y_test = datasets[grp]\n",
    "                        #         res, f_imp = custom_comb_validate(combined_rf, X, y, X_test, y_test, features=data.drop(columns=to_drop).columns.tolist(), scoring=scoring, feature_importance=feature_importance)\n",
    "                                \n",
    "                        #         meta_inf = {\n",
    "                        #             \"rep\": [rep] * CROSS_VAL, \"train_dataset\": [', '.join(map(str, comb))] * CROSS_VAL,\n",
    "                        #             \"val_dataset\": [grp] * CROSS_VAL, \"analysis\": [dataset_name] * CROSS_VAL,\n",
    "                        #             \"cv\": list(range(CROSS_VAL)), 'n_clients': [n_clients] * CROSS_VAL\n",
    "                        #         }\n",
    "                                \n",
    "                        #         res.update(meta_inf)\n",
    "                        #         f_imp.update(meta_inf)\n",
    "                        #         res_out.append(res)\n",
    "                        #         f_imp_out.append(f_imp)\n",
    "                                \n",
    "                        #     log_step(f\"validation\", start, dataset=comb, level=logging.DEBUG)\n",
    "\n",
    "                        \n",
    "                        #display(res_out[0])\n",
    "                        #display(res)\n",
    "\n",
    "                        # Write only the new results one by one in append mode\n",
    "                        for new_res, new_f_imp in zip(res_out[results_written:], f_imp_out[results_written:]):\n",
    "                            pickle.dump((new_res, new_f_imp), f)\n",
    "\n",
    "                        results_written += len(res_out[results_written:])\n",
    "                                    \n",
    "                        pbar.update(1)\n",
    "\n",
    "                    log_step(f\"{n_clients} client(s)\", client_start, level=logging.DEBUG)\n",
    "\n",
    "            log_step(f\"repetition\", rep_start, repetition=rep + 1, level=logging.WARNING)\n",
    "\n",
    "        # Final save\n",
    "        os.rename(save_path_unfinished, save_path)\n",
    "        log_step(\"writing final results\", start, level=logging.WARNING)\n",
    "\n",
    "    execution_log[\"end_time\"] = time.time()\n",
    "    execution_log[\"end_time_str\"] = datetime.fromtimestamp(execution_log[\"end_time\"]).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    execution_log[\"total_duration\"] = str(timedelta(seconds=int(execution_log[\"end_time\"] - start_time)))\n",
    "\n",
    "    log_step(\"finish simulation\", start_time, level=logging.INFO)\n",
    "    append_to_json(log_file, execution_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pickled_results(experiment, simulation_type, save_dir=save_dir, table_dir=table_dir, prefix='', **kwargs):\n",
    "    save_path = path_join(save_dir, f'{prefix}{experiment.name}_{simulation_type}_results.pkl')\n",
    "    results, f_imp = [], []\n",
    "    with open(save_path, 'rb') as f:\n",
    "        try:\n",
    "            while True:\n",
    "                res_out_tmp, f_imp_out_tmp = pickle.load(f)\n",
    "                results.append(res_out_tmp)\n",
    "                f_imp.append(f_imp_out_tmp)\n",
    "        except EOFError as e:   \n",
    "            pass\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if simulation_type in ['local', 'central']:\n",
    "        to_explode = results_df.columns.values.tolist()\n",
    "    else:\n",
    "        to_explode = list(compress(results_df.columns.to_list(),\n",
    "                    [type(_) == list for _ in results_df.iloc[0]]))\n",
    "    results_df = results_df.explode(column=to_explode).apply(pd.to_numeric, errors='ignore')\n",
    "    results_df = results_df.reset_index(drop=True)\n",
    "    \n",
    "    # write results to human readible files under /data/simulations/tables \n",
    "    out_dir = path.join(table_dir, f'{prefix}{simulation_type}_experiments', f'{experiment.name}.csv')\n",
    "    Path(out_dir).parent.mkdir(parents=True, exist_ok=True)\n",
    "    results_df.to_csv(out_dir, sep=',', index=False)\n",
    "    \n",
    "def load_results(experiment, simulation_type, save_dir=table_dir, prefix='', **kwargs):\n",
    "    path = Path(path_join(save_dir, f'{prefix}{simulation_type}_experiments', f'{experiment.name}.csv'))\n",
    "    if not path.exists():\n",
    "        print(f\"Could not find results for {simulation_type} - {experiment.name}. Trying to recreate them ... \")\n",
    "        \n",
    "        try:\n",
    "            format_pickled_results(experiment, simulation_type, prefix=prefix)    \n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            raise FileNotFoundError(f\"Recreation failed, Make sure the simulation was run successfull. {simulation_type} - {experiment.name}\", e)\n",
    "        \n",
    "    results_df = pd.read_csv(path)\n",
    "    return results_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(df, summary_df, simulation_type):\n",
    "#     df = df.groupby(by=['analysis', 'train_dataset', 'val_dataset', 'n_clients']).median().reset_index()\n",
    "    df['simulation_type'] = simulation_type\n",
    "    summary_df = pd.concat([summary_df, df]).reset_index(drop=True)\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_threaded(experiment, simulation_type, save_dir=save_dir, EXEC_MODE=EXEC_MODE, REPETITION=REPETITION, N_JOBS=N_JOBS, prefix='', **kwargs):\n",
    "    anno_df = pd.read_csv(Path(data_dir) / experiment.anno, index_col=0) if isinstance(experiment.anno, (str, Path)) else experiment.anno\n",
    "    counts_df = pd.read_csv(Path(data_dir) / experiment.count, index_col=0) if isinstance(experiment.count, (str, Path)) else experiment.counts\n",
    "\n",
    "    try:\n",
    "        simulate(\n",
    "            anno_df=anno_df, counts=counts_df, simulation_type=simulation_type, dataset_name=experiment.name, \n",
    "            groups=groups, save_path=save_dir, filename=f'{prefix}{experiment.name}_{simulation_type}_results.pkl',\n",
    "            exec_mode=EXEC_MODE, REPETITION=REPETITION, N_JOBS=N_JOBS, **kwargs\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {experiment.name} due to error: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    format_pickled_results(experiment, simulation_type, save_dir=save_dir, prefix=prefix, **kwargs)\n",
    "\n",
    "def run_simulation_with_threading(experiments, simulation_type, max_threads=N_JOBS, threads_per_job = 6, groups=groups, **kwargs):\n",
    "    num_cores = os.cpu_count() or 1\n",
    "    disable_telegram = False if is_server() else True\n",
    "\n",
    "    if max_threads is None or max_threads == -1:\n",
    "        max_threads = num_cores -2  # Use all available threads\n",
    "    elif max_threads == -2:\n",
    "        max_threads = num_cores // 2  # Use half of the available threads\n",
    "    else:\n",
    "        max_threads = min(max_threads, num_cores)  # Limit to the available threads\n",
    "        \n",
    "    max_concurrent_jobs = max_threads // threads_per_job\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_concurrent_jobs) as executor:\n",
    "        future_to_exp = {\n",
    "            executor.submit(\n",
    "                partial(simulate_threaded, exp, simulation_type, N_JOBS=threads_per_job, **kwargs)\n",
    "            ): exp for exp in experiments\n",
    "        }\n",
    "        with telegram_tqdm(\n",
    "            total=len(future_to_exp), desc=f'running {simulation_type}', \n",
    "            token=telegram_token, chat_id=telegram_chatid, disable=disable_telegram\n",
    "        ) as pbar:\n",
    "            for future in concurrent.futures.as_completed(future_to_exp):\n",
    "                exp = future_to_exp[future]\n",
    "                try:\n",
    "                    pbar.update(1)\n",
    "                except Exception as exc:\n",
    "                    print(f\"Experiment {exp.name} generated an exception: {exc}\")\n",
    "                    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_MODE = 'load' # 'redo'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_file = f\"execution_time_{timestamp}.json\"\n",
    "verbose = False\n",
    "\n",
    "# Run central simulations\n",
    "print(\"Running central simulations...\")\n",
    "run_simulation_with_threading(experiments, scoring=scoring, \n",
    "    simulation_type='central', feature_importance='', verbose=verbose, EXEC_MODE=EXEC_MODE, log_file=log_file)\n",
    "\n",
    "\n",
    "print(\"\\n-----------------------------\\n\")\n",
    "print(\"Running local simulations...\")\n",
    "# Run local simulations\n",
    "run_simulation_with_threading(experiments, scoring=scoring, \n",
    "    simulation_type='local', feature_importance='', verbose=verbose, EXEC_MODE=EXEC_MODE, log_file=log_file)\n",
    "\n",
    "print(\"\\n-----------------------------\\n\")\n",
    "print(\"Running combinations simulations...\")\n",
    "# Run combinations simulations\n",
    "run_simulation_with_threading(experiments, scoring=scoring, \n",
    "    simulation_type='combinations', feature_importance='', verbose=verbose, EXEC_MODE=EXEC_MODE, log_file=log_file)\n",
    "\n",
    "# # no fed simulation for now\n",
    "# print(\"\\n-----------------------------\\n\")\n",
    "# print(\"Running federated simulations...\")\n",
    "# # Run federated simulations\n",
    "# results_federated, features_federated = run_simulation_with_threading(experiments, simulation_type='federated', feature_importance='', verbose=True, EXEC_MODE=EXEC_MODE)\n",
    "\n",
    "print(\"\\n-----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_box(_df: pd.DataFrame, _central_df:pd.DataFrame|None = None, _local_df:pd.DataFrame|None = None,  metric=\"test_roc_auc\", \n",
    "                         title:str = \"federated analysis\", save_path:str=\"./data/simulations/figures/\", no_output:bool=False, palette='tab10', **kwargs):\n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'box', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True)  \n",
    "    \n",
    "    vmin, vmax = (0.0, 1.0) if metric != 'test_mcc' else (-0.5, 1.0)\n",
    "        \n",
    "    _df = _df.loc[_df['n_clients'] < 11]\n",
    "    \n",
    "    for analysis, a_df in _df.groupby(by=\"analysis\"):\n",
    "        # a_df = a_df.apply(pd.to_numeric, errors='ignore')\n",
    "        #display(a_df.groupby(by=[\"val_dataset\", 'n_clients']).median(numeric_only=True).T)\n",
    "        #if not no_output: display(a_df.groupby(by=['n_clients']).median(numeric_only=True).T)\n",
    "\n",
    "        plt.figure(figsize=(16,8))\n",
    "        p = sns.boxplot(data=a_df, x='n_clients', y=metric, hue='val_dataset', fliersize=1, palette=palette, **kwargs)\n",
    "        \n",
    "        if _central_df is not None:\n",
    "            plt.axhline(_central_df[metric].median(), color='r', linestyle='--')\n",
    "        \n",
    "        plt.ylim((vmin, vmax))\n",
    "        plt.legend(title='Validation Cohorts', bbox_to_anchor=(0.5, -0.1), loc='upper center', ncol=6)\n",
    "        plt.suptitle(f'{clean(analysis)} - {title}')\n",
    "        plt.xlabel(\"Number of Cohorts\")\n",
    "        plt.ylabel(clean(metric))\n",
    "        \n",
    "        plt.savefig(Path(target_path, f'{analysis}_{title.replace(\" \", \"_\")}.png'), bbox_inches='tight', dpi=300)\n",
    "        if no_output:\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_line_additive_cohort(_df: pd.DataFrame, _central_df:pd.DataFrame|None = None, _local_df:pd.DataFrame|None = None,  metric=\"test_roc_auc\",\n",
    "                                         title:str = \"federated analysis\", save_path:str=\"./data/simulations/figures/\", no_output:bool=False, palette='tab10', **kwargs):\n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'line_additive', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True)  \n",
    "    vmin, vmax = (0.0, 1.0) if metric != 'test_mcc' else (-0.5, 1.0)\n",
    "    for analysis, base_df in _df.groupby(by=\"analysis\"):\n",
    "        unique_cohorts = base_df.loc[base_df['n_clients'] == 1, 'train_dataset'].unique()   \n",
    "        for c in unique_cohorts:\n",
    "            a_df = base_df.apply(pd.to_numeric, errors='ignore')\n",
    "            a_df = a_df.loc[(a_df['train_dataset'].str.contains(c, regex=False)) & (a_df['n_clients'] <=2)]\n",
    "            \n",
    "            # Ensure train_dataset is treated as a categorical variable\n",
    "            a_df['train_dataset'] = a_df['train_dataset'].astype('category')\n",
    "            \n",
    "            median_mcc = a_df.groupby('train_dataset')[metric].median().sort_values()\n",
    "            ordered_train_dataset = [c] + [x for x in median_mcc.index if x != c]\n",
    "            \n",
    "            plt.figure(figsize=(12,8))\n",
    "            p = sns.pointplot(data=a_df, x='train_dataset', y=metric, hue='val_dataset', palette=palette, linestyles='', \n",
    "                              order=ordered_train_dataset, errorbar=None, dodge=False, markers='o', **kwargs)\n",
    "            \n",
    "            plt.ylim((vmin, vmax))\n",
    "            plt.legend(bbox_to_anchor=(1.02, 1), loc=2,)\n",
    "            plt.title(f'{clean(analysis)} {c} - {title}')\n",
    "            plt.xlabel(f\"cohort combinations\")\n",
    "            plt.ylabel(clean(metric))\n",
    "            \n",
    "            # Format x-tick labels\n",
    "            labels = [_.get_text() for _ in p.get_xticklabels()]\n",
    "            \n",
    "            new_labels = [c]\n",
    "            for _ in labels:\n",
    "                if _ == c:\n",
    "                    continue\n",
    "                \n",
    "                _ = _.replace(c, '').replace(', ', '')    \n",
    "                _ = f'{c}\\n+ {_}'\n",
    "                new_labels.append(_)\n",
    "            p.set_xticklabels(new_labels)\n",
    "            \n",
    "            plt.savefig(Path(target_path, f'{analysis}_{c}_{title.replace(\" \", \"_\")}.png'), bbox_inches='tight')\n",
    "            if no_output:\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_box_additive_cohort(_df: pd.DataFrame, _central_df:pd.DataFrame|None = None, _local_df:pd.DataFrame|None = None,  metric=\"test_roc_auc\", \n",
    "                                         title:str = \"federated analysis\", save_path:str=\"./data/simulations/figures/\", no_output:bool=False, palette='tab10', **kwargs):\n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'box_additive', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True)  \n",
    "    vmin, vmax = (0.0, 1.0) if metric != 'test_mcc' else (-0.5, 1.0)\n",
    "    for analysis, base_df in _df.groupby(by=\"analysis\"):\n",
    "        unique_cohorts = base_df.loc[base_df['n_clients'] == 1, 'train_dataset'].unique()   \n",
    "        for c in unique_cohorts:\n",
    "            a_df = base_df.apply(pd.to_numeric, errors='ignore')\n",
    "            a_df = a_df.loc[(a_df['train_dataset'].str.contains(c, regex=False)) & (a_df['n_clients'] <=2)]\n",
    "            \n",
    "            # Ensure train_dataset is treated as a categorical variable\n",
    "            a_df['train_dataset'] = a_df['train_dataset'].astype('category')\n",
    "            \n",
    "            median_mcc = a_df.groupby('train_dataset')[metric].median().sort_values()\n",
    "            ordered_train_dataset = [c] + [x for x in median_mcc.index if x != c]\n",
    "            \n",
    "            plt.figure(figsize=(12,8))\n",
    "            sns.boxplot(data=a_df, x='train_dataset', y=metric, color='lightgrey', fill=False, \n",
    "                              order=ordered_train_dataset, **kwargs)\n",
    "            p = sns.stripplot(data=a_df, x='train_dataset', y=metric, hue=\"val_dataset\", palette=palette, \n",
    "                              order=ordered_train_dataset, **kwargs)\n",
    "            plt.ylim((vmin, vmax))\n",
    "            plt.legend(bbox_to_anchor=(1.02, 1), loc=2)\n",
    "            plt.title(f'{clean(analysis)} {c} - {title}')\n",
    "            plt.xlabel(f\"cohort combinations\")\n",
    "            plt.ylabel(clean(metric))\n",
    "            \n",
    "            # Format x-tick labels\n",
    "            labels = [_.get_text() for _ in p.get_xticklabels()]\n",
    "            \n",
    "            new_labels = [c]\n",
    "            for _ in labels:\n",
    "                if _ == c:\n",
    "                    continue\n",
    "                \n",
    "                _ = _.replace(c, '').replace(', ', '')    \n",
    "                _ = f'{c}\\n+ {_}'\n",
    "                new_labels.append(_)\n",
    "            p.set_xticklabels(new_labels)\n",
    "            \n",
    "            plt.savefig(Path(target_path, f'{analysis}_{c}_{title.replace(\" \", \"_\")}.png'), bbox_inches='tight')\n",
    "            if no_output:\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_multi_box_additive_cohort(_df: pd.DataFrame, _central_df:pd.DataFrame|None = None, _local_df:pd.DataFrame|None = None, cohorts=[\"Brazil1\", \"China1\"],  metric=\"test_roc_auc\", \n",
    "                                         title:str = \"federated analysis\", save_path:str=\"./data/simulations/figures/\", no_output:bool=False, palette='tab10', **kwargs):\n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'box_additive_multi', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True)  \n",
    "    vmin, vmax = (0.0, 1.0) if metric != 'test_mcc' else (-0.5, 1.0)\n",
    "    _df = _df.apply(pd.to_numeric, errors='ignore')\n",
    "    _df = _df.loc[_df['n_clients'] <=2]\n",
    "    unique_cohorts = _df.loc[_df['n_clients'] == 1, 'train_dataset'].unique()   \n",
    "    analysis = _df.iloc[0][\"analysis\"]\n",
    "    \n",
    "    row, col = 1, len(cohorts)\n",
    "    gs = GridSpec(row, col)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    axes = [fig.add_subplot(gs[i, j]) for i in range(row) for j in range(col)]\n",
    "    \n",
    "    for i, c in enumerate(cohorts):\n",
    "        a_df = _df.loc[_df['train_dataset'].str.contains(c, regex=False)]\n",
    "        \n",
    "        # Ensure train_dataset is treated as a categorical variable\n",
    "        a_df['train_dataset'] = a_df['train_dataset'].astype('category')\n",
    "        \n",
    "        median_mcc = a_df.groupby('train_dataset')[metric].median().sort_values()\n",
    "        ordered_train_dataset = [c] + [x for x in median_mcc.index if x != c]\n",
    "        \n",
    "        sns.boxplot(data=a_df, x='train_dataset', y=metric, color='lightgrey', fill=False, \n",
    "                        order=ordered_train_dataset, ax=axes[i], **kwargs)\n",
    "        sns.stripplot(data=a_df, x='train_dataset', y=metric, hue=\"val_dataset\", palette=palette, \n",
    "                        order=ordered_train_dataset, ax=axes[i], **kwargs)\n",
    "        \n",
    "        axes[i].set_ylim((vmin, vmax))\n",
    "        axes[i].set_title(f'{c} vs. {c} + 1')\n",
    "        #axes[i].set_xlabel(f\"cohort combinations\")\n",
    "        axes[i].set_ylabel(clean(metric))\n",
    "        axes[i].legend().remove()\n",
    "        \n",
    "        # Format x-tick labels\n",
    "        labels = [_.get_text() for _ in axes[i].get_xticklabels()]\n",
    "        \n",
    "        new_labels = [c]\n",
    "        for _ in labels:\n",
    "            if _ == c:\n",
    "                continue\n",
    "            \n",
    "            _ = _.replace(c, '').replace(', ', '')    \n",
    "            _ = f'+ {_}'\n",
    "            new_labels.append(_)\n",
    "        axes[i].set_xticklabels(new_labels, rotation=-90)\n",
    "        \n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, title='Validation Cohorts', bbox_to_anchor=(0.5, -0.1), loc='upper center', ncol=6)\n",
    "    \n",
    "    fig.suptitle(f'{title}')\n",
    "    \n",
    "    for ax, letter in zip([*axes], ['A', 'B', 'C', 'D']):\n",
    "        ax.text(-0.13, 1.05, letter, transform=ax.transAxes, fontsize=24, fontweight='bold', va='top', ha='center')\n",
    "    \n",
    "    \n",
    "    plt.savefig(Path(target_path, f'{analysis}_{c}_{title.replace(\" \", \"_\")}.png'), bbox_inches='tight')\n",
    "    if no_output:\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_violin_additive_cohort(_df: pd.DataFrame, _central_df:pd.DataFrame|None = None, _local_df:pd.DataFrame|None = None,  metric=\"test_roc_auc\", \n",
    "                                         title:str = \"federated analysis\", save_path:str=\"./data/simulations/figures/\", no_output:bool=False, palette='tab10', **kwargs):\n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'violin_additive', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True)  \n",
    "    vmin, vmax = (0.0, 1.0) if metric != 'test_mcc' else (-0.5, 1.0)\n",
    "    for analysis, base_df in _df.groupby(by=\"analysis\"):\n",
    "        unique_cohorts = base_df.loc[base_df['n_clients'] == 1, 'train_dataset'].unique()   \n",
    "        for c in unique_cohorts:\n",
    "            a_df = base_df.apply(pd.to_numeric, errors='ignore')\n",
    "            a_df = a_df.loc[(a_df['train_dataset'].str.contains(c, regex=False)) & (a_df['n_clients'] <=2)]\n",
    "            \n",
    "            # Ensure train_dataset is treated as a categorical variable\n",
    "            a_df['train_dataset'] = a_df['train_dataset'].astype('category')\n",
    "            \n",
    "            median_mcc = a_df.groupby('train_dataset')[metric].median().sort_values()\n",
    "            ordered_train_dataset = [c] + [x for x in median_mcc.index if x != c]\n",
    "            \n",
    "            plt.figure(figsize=(12,8))\n",
    "            \n",
    "            sns.violinplot(data=a_df, x='train_dataset', y=metric, color=\"lightgrey\", fill=False, inner=None)\n",
    "            p = sns.stripplot(data=a_df, x='train_dataset', y=metric, hue=\"val_dataset\", palette=palette, \n",
    "                              order=ordered_train_dataset, **kwargs)\n",
    "            \n",
    "            plt.ylim((vmin, vmax))\n",
    "            plt.legend(bbox_to_anchor=(1.02, 1), loc=2,)\n",
    "            plt.title(f'{clean(analysis)} {c} - {title}')\n",
    "            plt.xlabel(f\"cohort combinations\")\n",
    "            plt.ylabel(clean(metric))\n",
    "            \n",
    "            # Format x-tick labels\n",
    "            labels = [_.get_text() for _ in p.get_xticklabels()]\n",
    "            \n",
    "            new_labels = [c]\n",
    "            for _ in labels:\n",
    "                if _ == c:\n",
    "                    continue\n",
    "                \n",
    "                _ = _.replace(c, '').replace(', ', '')    \n",
    "                _ = f'{c}\\n+ {_}'\n",
    "                new_labels.append(_)\n",
    "            p.set_xticklabels(new_labels)\n",
    "            \n",
    "            plt.savefig(Path(target_path, f'{analysis}_{c}_{title.replace(\" \", \"_\")}.png'), bbox_inches='tight')\n",
    "            if no_output:\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_double_linebox(_df: pd.DataFrame, _central_df: pd.DataFrame, _local_df: pd.DataFrame, _anno_df: None|pd.DataFrame = None, metric1: str = \"test_mcc\", metric2: str = \"test_f1\", \n",
    "                        title: str = \"double trouble\", save_path: str = \"./data/simulations/\", no_output: bool = False, as_line = False, palette = 'Accent', **kwargs):\n",
    "    target_path = Path(save_path) / 'figures' / 'double_linebox' / \"_\".join([metric1, metric2])\n",
    "    target_path.mkdir(exist_ok=True, parents=True)  \n",
    "    \n",
    "    vmin1, vmax1 = (-0.5, 1.0) \n",
    "    vmin2, vmax2 = (0.0, 1.0) \n",
    "    errorbar = kwargs.pop('errorbar', 'sd')    \n",
    "\n",
    "    # remove highest number of clients\n",
    "    _df =  _df.loc[_df['n_clients'] < _df['n_clients'].max()]\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    row, col = 2, 4\n",
    "    gs = GridSpec(row, col, width_ratios=[1, 3, 3, 4])\n",
    "    \n",
    "    axes = [fig.add_subplot(gs[i, j]) for i in range(row) for j in range(col)]\n",
    "    \n",
    "    # Ensure we have the correct number of axes\n",
    "    assert len(axes) == row * col, f\"Expected {row * col} axes, but got {len(axes)}\"\n",
    "    \n",
    "    # Plot for metric1\n",
    "    sns.boxplot(data=_central_df, y=metric1, ax=axes[0], color='r', **kwargs)\n",
    "    axes[0].set_title('Central\\nModel')\n",
    "    axes[0].set_ylabel(clean(metric1), size='large')\n",
    "    axes[0].set_xlabel(\"All Cohorts\")\n",
    "    axes[0].set_xticks([])\n",
    "    axes[0].set_ylim((vmin1, vmax1))\n",
    "    \n",
    "    sns.boxplot(data=_local_df.loc[_local_df['n_clients'] == 1], x='n_clients', y=metric1, hue='train_dataset', palette=palette, ax=axes[1], **kwargs)\n",
    "    \n",
    "    axes[1].axhline(_central_df[metric1].median(), color='r', linestyle='--')\n",
    "    axes[1].set_xticks([])\n",
    "    axes[1].set_ylim((vmin1, vmax1))\n",
    "    axes[1].set_xlabel(\"Individual Cohorts\\n(intra-cohort)\")\n",
    "    axes[1].set_ylabel(\"\")\n",
    "    axes[1].set_title('Local Models\\n')\n",
    "    axes[1].legend().remove()\n",
    "    \n",
    "    sns.boxplot(data=_df.loc[_df['n_clients'] == 1], x='n_clients', y=metric1, hue='train_dataset', palette=palette, ax=axes[2], **kwargs)\n",
    "    \n",
    "    axes[2].axhline(_central_df[metric1].median(), color='r', linestyle='--')\n",
    "    axes[2].set_xticks([])\n",
    "    axes[2].set_ylim((vmin1, vmax1))\n",
    "    axes[2].set_xlabel(\"Individual Cohorts\\n(inter-cohort)\")\n",
    "    axes[2].set_ylabel(\"\")\n",
    "    axes[2].set_title('Combined Models n=1\\n(by training cohort)')\n",
    "    axes[2].legend().remove()\n",
    "    \n",
    "    \n",
    "    sns.pointplot(data=_df.loc[_df['n_clients'] > 0], x='n_clients', y=metric1, hue='val_dataset', \n",
    "                  palette=palette, dodge=True, ax=axes[3], marker=\".\", markersize=10, markeredgewidth=3,\n",
    "                  err_kws={'linewidth': 0.5}, errorbar=errorbar, **kwargs)\n",
    "\n",
    "    axes[3].axhline(_central_df[metric1].median(), color='r', linestyle='--')\n",
    "    axes[3].set_ylim((vmin1, vmax1))\n",
    "    axes[3].set_ylabel(\"\")\n",
    "    axes[3].set_xlabel(\"Number of Cohorts\")\n",
    "    axes[3].set_title(\"Combined Models\\n(by validation cohort)\")\n",
    "    axes[3].legend().remove()\n",
    "    \n",
    "    handles, labels = axes[2].get_legend_handles_labels()\n",
    "    \n",
    "    # Plot for metric2\n",
    "    sns.boxplot(data=_central_df, y=metric2, ax=axes[4], color='r', **kwargs)\n",
    "    axes[4].set_ylabel(clean(metric2), size='large')\n",
    "    axes[4].set_xlabel(\"All Cohorts\")\n",
    "    axes[4].set_ylim((vmin2, vmax2))\n",
    "    axes[4].set_xticks([])\n",
    "    \n",
    "    sns.boxplot(data=_local_df.loc[_local_df['n_clients'] == 1], x='n_clients', y=metric2, hue='train_dataset', palette=palette, ax=axes[5], **kwargs)\n",
    "    \n",
    "    axes[5].axhline(_central_df[metric2].median(), color='r', linestyle='--')\n",
    "    axes[5].set_xticks([])\n",
    "    axes[5].set_ylim((vmin2, vmax2))\n",
    "    axes[5].set_xlabel(\"Individual Cohorts\\n(intra-cohort)\")\n",
    "    axes[5].set_ylabel(\"\")\n",
    "    axes[5].legend().remove()\n",
    "    \n",
    "    sns.boxplot(data=_df.loc[_df['n_clients'] == 1], x='n_clients', y=metric2, hue='train_dataset', palette=palette, ax=axes[6], **kwargs)\n",
    "    \n",
    "    axes[6].axhline(_central_df[metric2].median(), color='r', linestyle='--')\n",
    "    axes[6].set_xticks([])\n",
    "    axes[6].set_ylim((vmin2, vmax2))\n",
    "    axes[6].set_xlabel(\"Individual Cohorts\\n(inter-cohort)\")\n",
    "    axes[6].set_ylabel(\"\")\n",
    "    axes[6].legend().remove()\n",
    "    \n",
    "    sns.pointplot(data=_df.loc[_df['n_clients'] > 0], x='n_clients', y=metric2, hue='val_dataset', \n",
    "                  palette=palette, dodge=True, ax=axes[7], marker=\".\", markersize=10, markeredgewidth=3,\n",
    "                  err_kws={'linewidth': 0.5}, errorbar=errorbar, **kwargs)\n",
    "    \n",
    "    axes[7].axhline(_central_df[metric2].median(), color='r', linestyle='--')\n",
    "    axes[7].set_ylim((vmin2, vmax2))\n",
    "    axes[7].set_ylabel(\"\")\n",
    "    axes[7].set_xlabel(\"Number of Cohorts\")\n",
    "    axes[7].legend().remove()\n",
    "    \n",
    "    fig.legend(handles, labels, title='Validation Cohorts', bbox_to_anchor=(0.5, 0.02), loc='upper center', ncol=6)\n",
    "    \n",
    "    fig.suptitle(f'{title}')\n",
    "    \n",
    "    for ax, letter in zip([axes[0], axes[1], axes[2], axes[3]], ['A', 'B', 'C', 'D']):\n",
    "        ax.text(-0.1, 1.15, letter, transform=ax.transAxes, fontsize=24, fontweight='bold', va='top', ha='center')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    fig.savefig(Path(target_path, f'{title.replace(\" \", \"_\")}.png'), bbox_inches='tight')\n",
    "    \n",
    "    if _anno_df is not None:\n",
    "        fig.legends.clear()\n",
    "        \n",
    "        new_labels = []\n",
    "        for label in labels:\n",
    "            n_samples = _anno_df.loc[_anno_df['cohort_name'] == label].__len__()\n",
    "            new_labels.append(f\"{label} ({n_samples})\")\n",
    "        fig.legend(handles, new_labels, title='Validation Cohorts', bbox_to_anchor=(0.5, 0.02), loc='upper center', ncol=6)\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        fig.savefig(Path(target_path, f'{title.replace(\" \", \"_\")}_nsamples.png'), bbox_inches='tight')\n",
    "    \n",
    "    if no_output:\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_linebox(_df: pd.DataFrame, _central_df:pd.DataFrame, _local_df:pd.DataFrame, metric:str=\"test_roc_auc\", \n",
    "                             title:str = \"federated analysis\", save_path:str=\"./data/simulations/figures/\", no_output:bool=False, as_line=False, palette='Accent', **kwargs):\n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'linebox', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True)  \n",
    "    \n",
    "    vmin, vmax = (0.0, 1.0) if metric != 'test_mcc' else (-0.5, 1.0)\n",
    "    errorbar = kwargs.pop('errorbar', 'sd')    \n",
    "    for analysis, a_df in _df.groupby(by=\"analysis\"):\n",
    "        # a_df = a_df.apply(pd.to_numeric, errors='ignore')\n",
    "        #display(a_df.groupby(by=[\"val_dataset\", 'n_clients']).median(numeric_only=True).T)\n",
    "        #if not no_output: display(a_df.groupby(by=['n_clients']).median(numeric_only=True).T)\n",
    "\n",
    "        # remove highest number of clients\n",
    "        a_df =  a_df.loc[a_df['n_clients'] < a_df['n_clients'].max()]\n",
    "        \n",
    "        #plt.figure(figsize=[12,8])\n",
    "        fig = plt.figure(figsize=(12,8))\n",
    "        gs = GridSpec(1, 4, width_ratios=[1, 3, 3, 4])\n",
    "        axes = [fig.add_subplot(gs[i]) for i in range(4)]\n",
    "        \n",
    "        if _central_df is not None:\n",
    "            sns.boxplot(data=_central_df, y=metric, ax=axes[0], color='r', **kwargs)\n",
    "            axes[0].set_title('Central\\nModel')\n",
    "            axes[0].set_ylabel(clean(metric))\n",
    "            axes[0].set_xlabel(\"All Cohorts\")\n",
    "            axes[0].set_xticks([])\n",
    "            axes[0].set_ylim((vmin, vmax))\n",
    "        \n",
    "        sns.boxplot(data=_local_df.loc[_local_df['n_clients'] == 1], x='n_clients', y=metric, hue='train_dataset', palette=palette, ax=axes[1], **kwargs)\n",
    "        if _central_df is not None:\n",
    "            axes[1].axhline(_central_df[metric].median(), color='r', linestyle='--')\n",
    "        axes[1].set_xticks([])\n",
    "        axes[1].set_ylim((vmin, vmax))\n",
    "        axes[1].set_xlabel(\"Individual Cohorts\")\n",
    "        axes[1].set_ylabel(\"\")\n",
    "        axes[1].set_title('Local Models\\n')\n",
    "        axes[1].legend().remove()\n",
    "        \n",
    "        sns.boxplot(data=_df.loc[_df['n_clients'] == 1], x='n_clients', y=metric, hue='train_dataset', palette=palette, ax=axes[2], **kwargs)\n",
    "        if _central_df is not None:\n",
    "            axes[2].axhline(_central_df[metric].median(), color='r', linestyle='--')\n",
    "        axes[2].set_xticks([])\n",
    "        axes[2].set_ylim((vmin, vmax))\n",
    "        axes[2].set_xlabel(\"Individual Cohorts\")\n",
    "        axes[2].set_ylabel(\"\")\n",
    "        axes[2].set_title('Combined Models n=1\\n(by training cohort)')\n",
    "        axes[2].legend().remove()\n",
    "        \n",
    "        \n",
    "        sns.pointplot(data=_df.loc[_df['n_clients'] > 0], x='n_clients', y=metric, hue='val_dataset', \n",
    "                    palette=palette, dodge=True, ax=axes[3], marker=\".\", markersize=10, markeredgewidth=3,\n",
    "                    err_kws={'linewidth': 0.5}, errorbar=errorbar, **kwargs)\n",
    "        if _central_df is not None:\n",
    "            axes[3].axhline(_central_df[metric].median(), color='r', linestyle='--')\n",
    "        axes[3].set_ylim((vmin, vmax))\n",
    "        axes[3].set_ylabel(\"\")\n",
    "        axes[3].set_xlabel(\"Number of Cohorts\")\n",
    "        axes[3].set_title(\"Combined Models\\n(by validation cohort)\")\n",
    "        axes[3].legend().remove()\n",
    "        \n",
    "        handles, labels = axes[3].get_legend_handles_labels()\n",
    "    \n",
    "        plt.legend().remove()\n",
    "        #ax2.set_xticks([int(_) for _ in range(1, len(a_df['n_clients'].unique()))])\n",
    "        \n",
    "        # plt.legend(bbox_to_anchor=(1.05, 1), loc=2,)\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, title='Validation Cohorts', bbox_to_anchor=(0.5, 0.02), loc='upper center', ncol=6)\n",
    "\n",
    "            \n",
    "        plt.title(\"Performance of all Evaluation Steps\\nby validation cohort\")\n",
    "        fig.suptitle(f'{clean(analysis)} - {title}')\n",
    "        \n",
    "        plt.savefig(Path(target_path, f'{analysis}_{title.replace(\" \", \"_\")}.png'), bbox_inches='tight')\n",
    "        if no_output:\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_by_train_box(_df: pd.DataFrame, _central_df:pd.DataFrame|None = None, _local_df:pd.DataFrame|None = None,  metric=\"test_roc_auc\", \n",
    "                                  title:str = \"federated analysis\", save_path:str=\"./data/simulations/figures/\", no_output:bool=False, palette='Accent', **kwargs):\n",
    "    \n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'box_train', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True)    \n",
    "    vmin, vmax = (0.0, 1.0) if metric != 'test_mcc' else (-0.5, 1.0)\n",
    "    if _local_df is not None:\n",
    "        _local_df.loc[:, 'n_clients'] = 0\n",
    "        _df = pd.concat([_df, _local_df]).reset_index(drop=True)\n",
    "        #display(_df.n_clients.value_counts())\n",
    "    for analysis, a_df in _df.groupby(by=\"analysis\"):\n",
    "\n",
    "        #plt.figure(figsize=[12,8])\n",
    "        p = sns.boxplot(data=a_df, x='n_clients', y=metric, hue='train_dataset', palette=palette, **kwargs)\n",
    "        \n",
    "        if _central_df is not None:\n",
    "            plt.axhline(_central_df[metric].median(), color='r', linestyle='--')\n",
    "                    \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "            ticklabels = []\n",
    "            if _local_df is not None:\n",
    "                ticklabels.append('local')\n",
    "                \n",
    "            ticklabels.extend([str(_) for _ in range(1, len(a_df['n_clients'].unique()))])\n",
    "            p.set_xticklabels(ticklabels)\n",
    "        \n",
    "        plt.ylim((vmin, vmax))\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2,)\n",
    "        plt.title(f'{clean(analysis)} - {title}')\n",
    "        plt.xlabel(\"number of training clients\")\n",
    "        plt.ylabel(clean(metric))\n",
    "        \n",
    "        plt.savefig(Path(target_path, f'{analysis}_{title.replace(\" \", \"_\")}.png'), bbox_inches='tight')\n",
    "        if no_output:\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box_local(local_df, metric=\"test_roc_auc\", title=\"baseline\", no_output=False, save_dir=save_dir, palette='Accent', **kwargs):\n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'local', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True)    \n",
    "\n",
    "    vmin, vmax = (0.0, 1.0) if metric != 'test_mcc' else (-0.5, 1.0)\n",
    "    \n",
    "    for analysis, a_df in local_df.groupby(by=\"analysis\"):\n",
    "        # a_df = a_df.apply(pd.to_numeric, errors='ignore')\n",
    "        #display(a_df.groupby(by=[\"val_dataset\", 'n_clients']).median(numeric_only=True).T)\n",
    "        #if not no_output: display(a_df.groupby(by=['n_clients']).median(numeric_only=True).T)\n",
    "\n",
    "        #plt.figure(figsize=[12,8])\n",
    "        p = sns.boxplot(data=a_df, x='val_dataset', y=metric, hue='val_dataset', palette=palette, **kwargs)\n",
    "        \n",
    "        plt.ylim((vmin, vmax))\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2,)\n",
    "        plt.title(f'{clean(analysis)} - {title} ')\n",
    "        plt.xlabel(\"cohort\")\n",
    "        plt.ylabel(clean(metric))\n",
    "        plt.xticks(rotation=45, ha='right') \n",
    "        plt.savefig(Path(target_path, f'{analysis}_{metric}_baseline.png'), bbox_inches='tight')\n",
    "        \n",
    "        if no_output:\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_heatmap(comb_df, local_df, show_core=False, metric='test_roc_auc', title_prefix:str = '', no_output=False, \n",
    "                             save_dir = save_dir, max_clients=1, palette='vlag', **kwargs):\n",
    "\n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'heatmaps', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True)    \n",
    "    \n",
    "    if local_df is not None:\n",
    "        _df = pd.concat([comb_df, local_df]).reset_index(drop=True)\n",
    "    else:\n",
    "        _df = comb_df\n",
    "    \n",
    "    for analysis, _df in _df.groupby(by='analysis'):\n",
    "        \n",
    "        save_title = f'median_{title_prefix}{\"_\" if title_prefix else \"\"}{metric}_{analysis}_{max_clients}clients'\n",
    "        \n",
    "        if max_clients > 1:\n",
    "            _df = _df.loc[_df['n_clients'] == max_clients]\n",
    "        else:\n",
    "            _df = _df.loc[_df['n_clients'] <=max_clients]\n",
    "            \n",
    "        if not show_core:\n",
    "            _df = _df.loc[(_df['val_dataset'] != 'core') & (_df['train_dataset'] != 'core')]\n",
    "        \n",
    "        _df = generate_heatmap_tables(_df, metric=metric, title=save_title, save_dir=save_dir)\n",
    "        if metric in ['test_mcc']:\n",
    "            vmin, vmax = -1.0, 1.0\n",
    "        else:\n",
    "            vmin, vmax = 0.0, 1.0\n",
    "            \n",
    "        square = True if max_clients == 1 else False\n",
    "        \n",
    "        match max_clients:\n",
    "            case 1: figsize, fraction = (8, 8),  0.15\n",
    "            case 2: figsize, fraction = (8, 10), 0.1\n",
    "            case 3: figsize, fraction = (8, 36), 0.03\n",
    "            case 4: figsize, fraction = (8, 56), 0.03\n",
    "            case 5: figsize, fraction = (8, 112), 0.03\n",
    "            case 6: figsize, fraction = (10, 180), 0.03\n",
    "            case 7: figsize, fraction = (10, 112), 0.03\n",
    "            case 8: figsize, fraction = (10, 56), 0.03\n",
    "            case 9: figsize, fraction = (12, 36), 0.03\n",
    "            case 10: figsize, fraction = (12, 10), 0.1\n",
    "            case 11: figsize, fraction = (14,8), 0.15\n",
    "            case _: figsize, fraction = (8, 40), 0.01\n",
    "            \n",
    "        plt.figure(figsize=figsize)     \n",
    "        ax = sns.heatmap(data=_df, annot=True, fmt='.2f', annot_kws={'size':8}, cbar_kws={'shrink': 0.5, 'fraction':fraction}, vmin=vmin, vmax=vmax, cmap=palette, square=square, **kwargs)\n",
    "        title = f'{title_prefix}{clean(analysis)}: mean {metric}{f\" {max_clients} train cohorts\" if max_clients > 1 else \"\"}'\n",
    "        if max_clients > 1:\n",
    "            ax.set_yticks([x + 0.5 for x in range(_df.shape[0])])\n",
    "            ax.set_yticklabels(_df.index, ha='right', size=6)\n",
    "            #ax.tick_params(axis='y', which='both', pad=max([len(_) for _ in _df.index.tolist()])*3.5)\n",
    "        else:\n",
    "            for i in range(min(_df.shape)):\n",
    "                ax.add_patch(Rectangle((i, i), 1, 1, fill=False, edgecolor='yellow', lw=3))\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.ylabel(\"training dataset\")\n",
    "        plt.xlabel(\"validation dataset\")\n",
    "        #plt.xticks(rotation=30, ha='right')\n",
    "        plt.xticks()\n",
    "        plt.yticks()\n",
    "        \n",
    "        plt.tight_layout() \n",
    "        \n",
    "        # Dynamically calculate the required space for y-ticklabels\n",
    "        renderer = plt.gcf().canvas.get_renderer()\n",
    "        ytick_labels = ax.get_yticklabels()\n",
    "        max_label_width = max([label.get_window_extent(renderer=renderer).width for label in ytick_labels])\n",
    "\n",
    "        # Convert label width from display units to figure units\n",
    "        label_width_inch = max_label_width / plt.gcf().dpi\n",
    "        current_fig_width = plt.gcf().get_size_inches()[0]\n",
    "        left_margin = label_width_inch / current_fig_width + 0.05  # Adding a small padding\n",
    "\n",
    "        # Ensure the left margin has enough space for y-tick labels\n",
    "        plt.subplots_adjust(left=left_margin)\n",
    "\n",
    "        \n",
    "        plt.savefig(Path.joinpath(target_path, f'{save_title}.png'), bbox_inches='tight')\n",
    "\n",
    "        if no_output:\n",
    "            plt.close()\n",
    "        else:    \n",
    "            plt.show()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_heatmap_additive_cohort(comb_df, local_df, show_core=False, metric='test_roc_auc', title: str|None = None, no_output=False, \n",
    "                                             save_dir = save_dir, max_clients=1, palette='vlag', **kwargs):\n",
    "    gen_title = True if title is None else False\n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'heatmaps_additive', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True)    \n",
    "    df = comb_df\n",
    "    df = df.loc[df['n_clients'] <= max_clients]\n",
    "    \n",
    "    for analysis, _df in df.groupby(by='analysis'):\n",
    "            \n",
    "        unique_cohorts = _df.loc[_df['n_clients'] == 1, 'train_dataset'].unique()   \n",
    "        for c in unique_cohorts:\n",
    "            c_df = _df.loc[_df['train_dataset'].str.contains(c, regex=False)]\n",
    "            \n",
    "            c_df = c_df.loc[c_df['n_clients'] <=max_clients]\n",
    "            if not show_core:\n",
    "                c_df = c_df.loc[(c_df['val_dataset'] != 'core') & (c_df['train_dataset'] != 'core')]\n",
    "            c_df = c_df.loc[:, ['train_dataset', 'val_dataset', metric]]\n",
    "            c_df = c_df.groupby(by=['train_dataset', 'val_dataset']).median().reset_index()\n",
    "            c_df = c_df.pivot(index='train_dataset', columns='val_dataset', values=metric)\n",
    "            c_df = c_df.astype(np.float32)\n",
    "            \n",
    "            # Sort rows so that the row with train_dataset=c is the first row\n",
    "            c_df = c_df.reindex([c] + [x for x in c_df.index if x != c])\n",
    "            if c_df.empty:\n",
    "                print(f\"No data available for analysis {analysis} and cohort {c}. Skipping plot.\")\n",
    "                continue   \n",
    "            \n",
    "            if metric in ['test_mcc']:\n",
    "                vmin, vmax = -1.0, 1.0\n",
    "            else:\n",
    "                vmin, vmax = 0.0, 1.0\n",
    "            \n",
    "            try:\n",
    "                plt.figure(figsize=(8,8))\n",
    "                \n",
    "                sns.heatmap(data=c_df, annot=True, fmt='.2f', annot_kws={'size':8}, square=True, vmin=vmin, vmax=vmax, cmap=palette)\n",
    "                if gen_title:\n",
    "                    title = f'{clean(analysis)} {c}: mean {metric}{f\" {max_clients} train cohorts\" if max_clients > 1 else \"\"}'\n",
    "\n",
    "\n",
    "                plt.title(title)\n",
    "                plt.ylabel(\"training dataset\")\n",
    "                plt.xlabel(\"validation dataset\")\n",
    "                \n",
    "                plt.tight_layout()  \n",
    "            \n",
    "                plt.savefig(Path.joinpath(target_path, f'mean_{metric}_{analysis}_{c}.png'))\n",
    "                if no_output:\n",
    "                    plt.close('all')\n",
    "                else:    \n",
    "                    plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to plot for analysis {analysis} and cohort {c}: {e}\")\n",
    "                traceback.print_exc()\n",
    "            finally:\n",
    "                plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_significance_facetgrid(_df, group='val_dataset', metric='test_mcc', title=None, cohort_order=None, no_output=False, save_dir: str|Path = save_dir, palette='Accent'):\n",
    "    if title is None:\n",
    "        title = f\"Significance of {clean(metric)} change with increased number of cohorts - Welch's t-test\"\n",
    "    target_path = Path(save_dir) / 'figures' / 'significance_grid' / metric\n",
    "    target_path.mkdir(exist_ok=True, parents=True)\n",
    "    csv_output_path = Path(table_dir) / 'significance_analysis' \n",
    "    csv_output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    gen_order = True if cohort_order is None else False\n",
    "    if isinstance(palette, list):\n",
    "        palette = palette[1:] + palette[:1]\n",
    "\n",
    "    results = []  # To store t-test results\n",
    "\n",
    "    for analysis, a_df in _df.groupby(by=\"analysis\"):\n",
    "        if gen_order:\n",
    "            cohort_order = sorted(a_df[group].unique())\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        g = sns.FacetGrid(a_df, col=group, col_wrap=4, height=2, aspect=1.5, col_order=cohort_order, hue_order=cohort_order,\n",
    "                          sharex=False, sharey=False, hue=group, palette=palette)\n",
    "        g.map_dataframe(sns.boxplot, x='n_clients', y=metric, fliersize=1)\n",
    "        g.set(ylim=(-0.5, 1.0))\n",
    "\n",
    "        def annotate_significance_direct(data, ax, x='n_clients', y=metric):\n",
    "            clients = sorted(data[x].unique())\n",
    "\n",
    "            for i, (c1, c2) in enumerate(zip(clients[:-1], clients[1:])):\n",
    "                data1 = data[data[x] == c1][y]\n",
    "                data2 = data[data[x] == c2][y]\n",
    "\n",
    "                t_stat, p_val = ttest_ind(data1, data2, nan_policy='omit', equal_var=False)\n",
    "                dof = len(data1) + len(data2) - 2\n",
    "\n",
    "                conf_interval = (\n",
    "                    (data1.mean() - data2.mean()) - 1.96 * (data1.std() / len(data1)**0.5 + data2.std() / len(data2)**0.5),\n",
    "                    (data1.mean() - data2.mean()) + 1.96 * (data1.std() / len(data1)**0.5 + data2.std() / len(data2)**0.5)\n",
    "                )\n",
    "\n",
    "                corrected_results = multipletests([p_val], alpha=0.05, method='bonferroni')\n",
    "                adjusted_pvals = corrected_results[1][0]\n",
    "\n",
    "                annotation = '***' if adjusted_pvals < 0.001 else '**' if adjusted_pvals < 0.01 else '*' if adjusted_pvals < 0.05 else 'ns'\n",
    "                y_max = max(data1.max(), data2.max()) + 0.08\n",
    "\n",
    "                ax.plot([i+0.1, i + 1 - 0.1], [y_max, y_max], color='black')\n",
    "                ax.plot([i+0.1, i+0.1], [y_max - 0.02, y_max], color='black')\n",
    "                ax.plot([i + 1 - 0.1, i + 1 - 0.1], [y_max - 0.02, y_max], color='black')\n",
    "                ax.text((i + 0.5), y_max + 0.03, annotation, ha='center', color='black', size='small')\n",
    "\n",
    "                results.append({\n",
    "                    'Analysis': analysis,\n",
    "                    'Group_1': c1,\n",
    "                    'Group_2': c2,\n",
    "                    't_statistic': t_stat,\n",
    "                    'p_value': p_val,\n",
    "                    'Adjusted_p_value': adjusted_pvals,\n",
    "                    'Degrees_of_Freedom': dof,\n",
    "                    '95%_Confidence_Interval': conf_interval\n",
    "                })\n",
    "\n",
    "        for ax, val_dataset in zip(g.axes.flat, cohort_order):\n",
    "            subset = a_df[a_df[group] == val_dataset]\n",
    "            if not subset.empty:\n",
    "                annotate_significance_direct(subset, ax)\n",
    "\n",
    "        g.set_titles(col_template=\"{col_name}\")\n",
    "\n",
    "        legend_elements = [\n",
    "            Line2D([0], [0], color='black', lw=0, label='Significance:'),\n",
    "            Line2D([0], [0], color='black', lw=0, label='*** < 0.001'),\n",
    "            Line2D([0], [0], color='black', lw=0, label='** < 0.01'),\n",
    "            Line2D([0], [0], color='black', lw=0, label='* < 0.05'),\n",
    "            Line2D([0], [0], color='black', lw=0, label='ns > 0.05')\n",
    "        ]\n",
    "\n",
    "        g.fig.legend(handles=legend_elements, loc='upper left', framealpha=0, bbox_to_anchor=(1, 0.92))\n",
    "        g.set_ylabels(clean(metric))\n",
    "        g.set_xlabels(\"Number of Clients\")\n",
    "        if title:\n",
    "            plt.subplots_adjust(top=0.9)\n",
    "            g.fig.suptitle(title, fontsize=16)\n",
    "\n",
    "        g.tight_layout()\n",
    "\n",
    "        plt.savefig(target_path / f'significance_comb_{metric}_{analysis}.png', bbox_inches='tight', dpi=300)\n",
    "        if no_output:\n",
    "            plt.close(g.fig)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    # Write results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(csv_output_path / f'significance_analysis_{metric}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_summary_performance(df:pd.DataFrame, metric:str=\"test_roc_auc\", errorbar:str|None=None, save_dir:str|Path=save_dir, title:str=\"summary comparison\", prefix='', cohort_order=None, palette=\"Accent\", no_output=False, **kwargs):\n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'summary', 'comparison', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True) \n",
    "    #df = df.groupby(by=['simulation_type', 'analysis', 'n_clients']).median(numeric_only=True).reset_index()\n",
    "    if prefix:\n",
    "        prefix += \" \" if not prefix.endswith(\" \") else \"\"\n",
    "    title = f\"{prefix}{title}\"\n",
    "    \n",
    "    vmin, vmax = (0.0, 1.0) if metric != 'test_mcc' else (-0.5, 1.0)\n",
    "    dodge = True if len(df['analysis'].unique().tolist()) > 1 else False\n",
    "\n",
    "    df = df.loc[df[\"simulation_type\"] == \"combinations\"]\n",
    "    df = df.loc[df['n_clients'] < df['n_clients'].max()]\n",
    "    \n",
    "    df_0 = df.loc[df['analysis'].str.contains(\"_0\")]\n",
    "    df_0_0 = df_0.loc[~df_0['analysis'].str.contains(\"_batch_\")]\n",
    "    df_0_1 = df_0.loc[df_0['analysis'].str.contains(\"_batch_\")] \n",
    "    df_10 = df.loc[df['analysis'].str.contains(\"_10\")]\n",
    "    df_1_0 = df_10.loc[~df_10['analysis'].str.contains(\"_batch_\")]\n",
    "    df_1_1 = df_10.loc[df_10['analysis'].str.contains(\"_batch_\")] \n",
    "    \n",
    "    \n",
    "    palette = sns.color_palette(palette, 12)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12), sharey=False, sharex=False) \n",
    "    \n",
    "    sns.pointplot(data=df_0_0, x='n_clients', y=metric, hue='analysis', ax=axes[0][0],\n",
    "                palette=palette, dodge=dodge, marker=\".\", markersize=10, markeredgewidth=3,\n",
    "                err_kws={'linewidth':0.5}, errorbar=errorbar, hue_order=cohort_order, **kwargs)  \n",
    "\n",
    "    sns.pointplot(data=df_0_1, x='n_clients', y=metric, hue='analysis', ax=axes[0][1],\n",
    "                palette=palette, dodge=dodge, marker=\".\", markersize=10, markeredgewidth=3,\n",
    "                err_kws={'linewidth':0.5}, errorbar=errorbar, hue_order=cohort_order, **kwargs)  \n",
    "    \n",
    "    sns.pointplot(data=df_1_0, x='n_clients', y=metric, hue='analysis', ax=axes[1][0],\n",
    "                palette=palette, dodge=dodge, marker=\".\", markersize=10, markeredgewidth=3, linestyles='--',\n",
    "                err_kws={'linewidth':0.5}, errorbar=errorbar, hue_order=cohort_order, **kwargs)  \n",
    "    \n",
    "    sns.pointplot(data=df_1_1, x='n_clients', y=metric, hue='analysis', ax=axes[1][1],\n",
    "                palette=palette, dodge=dodge, marker=\".\", markersize=10, markeredgewidth=3, linestyles='--',\n",
    "                err_kws={'linewidth':0.5}, errorbar=errorbar, hue_order=cohort_order, **kwargs)  \n",
    "    \n",
    "    \n",
    "    for ax in axes.flat:\n",
    "        ax.set_ylim((vmin, vmax))\n",
    "        ax.set_xlabel(\"number of cohorts\")\n",
    "        ax.set_ylabel(clean(metric))\n",
    "        ax.legend().remove()\n",
    "        \n",
    "    axes[0][0].set_title(f\"No Batch Correction - No Filter\")\n",
    "    axes[0][1].set_title(f\"Batch Correction - No Filter\")\n",
    "    axes[1][0].set_title(f\"No Batch Correction - 10% Filter\")\n",
    "    axes[1][1].set_title(f\"Batch Correction - 10% Filter\")\n",
    "        \n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # Collect handles and labels from all subplots\n",
    "    handles, labels = [], []\n",
    "    for ax in axes.flat:\n",
    "        for handle, label in zip(*ax.get_legend_handles_labels()):\n",
    "            if label not in labels:\n",
    "                handles.append(handle)\n",
    "                labels.append(label)\n",
    "    \n",
    "    # Create custom legend handles\n",
    "    custom_handles = []\n",
    "    for handle, label in zip(handles, labels):\n",
    "        linestyle = '--' if '10' in label else '-'\n",
    "        color = handle.get_color()\n",
    "        custom_handles.append(Line2D([0], [0], color=color, linestyle=linestyle, marker='.', markersize=0, markeredgewidth=3))\n",
    "    \n",
    "    # Create a shared legend\n",
    "    legend = fig.legend(custom_handles, labels, title='Dataset Configurations', bbox_to_anchor=(0.5, 0.02), loc='upper center', ncol=4)    \n",
    "    \n",
    "    # param_box = f\"\"\"\n",
    "    # Model Training Parameters\n",
    "    # -------------------------\n",
    "    # N_ESTIMATOR:       {N_ESTIMATOR}\n",
    "    # MAX_DEPTH:         {MAX_DEPTH}\n",
    "    # CRITERION:         {CRITERION}\n",
    "    # MIN_SAMPLES_LEAF:  {MIN_SAMPLES_LEAF}\n",
    "    # MAX_FEATURES:      {MAX_FEATURES}\n",
    "    # MAX_SAMPLES:       {MAX_SAMPLES}\n",
    "    # CROSS_VAL:         {CROSS_VAL}\n",
    "    # CLASS_WEIGHT:      {CLASS_WEIGHT}\n",
    "    # \"\"\"\n",
    "    # # Transform legend coordinates to figure coordinates\n",
    "    # legend_bbox = legend.get_window_extent().transformed(fig.transFigure.inverted()).ymax\n",
    "    \n",
    "    # # Add the text box to the plot, aligned with the top of the legend\n",
    "    # plt.text(0.99, legend_bbox, param_box, transform=fig.transFigure, fontsize=10, family=\"monospace\",\n",
    "    #          va='top', ha='left', bbox=dict(boxstyle=\"round,pad=0.5\", edgecolor=\"darkgrey\", facecolor=\"white\"))\n",
    "    \n",
    "    plt.savefig(Path(target_path, f'summary_{title.replace(\" \", \"_\")}.png'), bbox_inches='tight')\n",
    "    if no_output:\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_selected_summary_performance(df:pd.DataFrame, metric:str=\"test_roc_auc\", errorbar:str|None=None, save_dir:str|Path=save_dir, title:str=\"select summary comparison\", prefix='', cohort_order=None, palette=\"Accent\", no_output=False, **kwargs):\n",
    "    target_path = Path.joinpath(Path(save_dir), 'figures', 'summary', 'select_comparison', metric)\n",
    "    target_path.mkdir(exist_ok=True, parents=True) \n",
    "    #df = df.groupby(by=['simulation_type', 'analysis', 'n_clients']).median(numeric_only=True).reset_index()\n",
    "    if prefix:\n",
    "        prefix += \" \" if not prefix.endswith(\" \") else \"\"\n",
    "    title = f\"{prefix}{title}\"\n",
    "\n",
    "    df = df.loc[df[\"simulation_type\"] == \"combinations\"]\n",
    "    df = df.loc[df['n_clients'] < df['n_clients'].max()]\n",
    "    df = df.loc[~df[\"analysis\"].str.contains(\"func_\")]\n",
    "    \n",
    "    vmin, vmax = (0.0, 1.0) if metric != 'test_mcc' else (-0.5, 1.0)\n",
    "    dodge = True if len(df['analysis'].unique().tolist()) > 1 else False\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.pointplot(data=df, x='n_clients', y=metric, hue='analysis',\n",
    "                palette=palette, dodge=dodge, marker=\".\", markersize=10, markeredgewidth=3,\n",
    "                err_kws={'linewidth':0.5}, errorbar=errorbar, hue_order=cohort_order, **kwargs)  \n",
    "    \n",
    "    plt.ylim((vmin, vmax))\n",
    "    plt.xlabel(\"number of cohorts\")\n",
    "    plt.ylabel(clean(metric))\n",
    "    plt.legend()        \n",
    "    plt.title(title)\n",
    "\n",
    "    \n",
    "    # Create custom legend handles\n",
    "    # custom_handles = []\n",
    "    # for handle, label in zip(handles, labels):\n",
    "    #     linestyle = '--' if '10' in label else '-'\n",
    "    #     color = handle.get_color()\n",
    "    #     custom_handles.append(Line2D([0], [0], color=color, linestyle=linestyle, marker='.', markersize=0, markeredgewidth=3))\n",
    "    \n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    labels = [clean(_) for _ in labels]\n",
    "    # Create a shared legend\n",
    "    plt.legend(handles, labels, title='Dataset Configurations', bbox_to_anchor=(0.5, -0.1), loc='upper center', ncol=4)   \n",
    "\n",
    "    plt.savefig(Path(target_path, f'summary_{title.replace(\" \", \"_\")}.png'), bbox_inches='tight')\n",
    "    if no_output:\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(_df, filter=None):\n",
    "    # remove samples from filter list\n",
    "    if filter is not None:\n",
    "        for f in filter:\n",
    "            _df = _df.loc[~_df['train_dataset'].str.contains(f, regex=False)]\n",
    "            _df = _df.loc[_df['val_dataset'] != f]\n",
    "            \n",
    "    return _df\n",
    "\n",
    "def split_data_by_train(_df, filter = None):\n",
    "    _df = filter_data(_df, filter)\n",
    "    _out_df = _df.copy()\n",
    "    \n",
    "    _df = _df['train_dataset'].str.split(', ').apply(pd.Series, 1).stack()\n",
    "    _df.index = _df.index.droplevel(-1) # to line up with df's index\n",
    "    _df.name = 'train_dataset' # needs a name to join\n",
    "    \n",
    "    _out_df = _out_df.drop(columns='train_dataset').join(_df)\n",
    "        \n",
    "    return _out_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame()\n",
    "\n",
    "palette = sns.color_palette(\n",
    "    ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c', '#98df8a', '#9467bd', '#c5b0d5', '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5']\n",
    ")\n",
    "\n",
    "cohort_order = ['Austria1', 'Brazil1', 'China1', 'China3', 'China5', 'France1', 'Germany1', 'Germany2', 'Italy1', 'Japan1', 'USA1', 'USA2']\n",
    "disable_telegram = False if is_server() else True\n",
    "\n",
    "def plot_results_for_experiment(experiment, metrics=['test_mcc'], palette='Accent', cohort_order=None):\n",
    "    \n",
    "    results_central_df = load_results(experiment, \"central\")\n",
    "    results_local_df = load_results(experiment, \"local\")\n",
    "    results_comb_df = load_results(experiment, \"combinations\")\n",
    "    \n",
    "    # split samples by train and build fed-like datasets\n",
    "    train_split_results_comb_df = split_data_by_train(results_comb_df)\n",
    "    fedlike_results_comb_df = split_data_by_train(results_comb_df, filter=['Brazil1', 'France1', 'USA1', 'USA2'])\n",
    "    fedlike_results_central_df = filter_data(results_central_df, filter=['Brazil1', 'France1', 'USA1', 'USA2'])\n",
    "    fedlike_results_local_df = filter_data(results_local_df, filter=['Brazil1', 'France1', 'USA1', 'USA2'])\n",
    "\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Suppress warnings and logging INFO messages\n",
    "    with warnings.catch_warnings():\n",
    "        # Suppress INFO level logging for matplotlib\n",
    "        logging.getLogger('matplotlib').setLevel(logging.ERROR)\n",
    "        \n",
    "        for m1,m2 in [('test_roc_auc', 'test_f1'), ('test_mcc', 'test_accuracy')]:\n",
    "            plot_double_linebox(results_comb_df, results_central_df, results_local_df, metric1=m1, metric2=m2, \n",
    "                                title=f\"{clean(experiment.name)} {m1} {m2} comb analysis - line by eval\",  \n",
    "                                errorbar='sd', no_output=True, palette=palette, hue_order=cohort_order, _anno_df = df_anno)\n",
    "            plot_double_linebox(results_comb_df, results_central_df, results_local_df, metric1=m1, metric2=m2, \n",
    "                                title=f\"{clean(experiment.name)} {m1} {m2} comb analysis - line by eval - no errorbar\", \n",
    "                                errorbar=None, no_output=True, palette=palette, hue_order=cohort_order, _anno_df = df_anno)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            plot_significance_facetgrid(results_comb_df, metric=metric, cohort_order=cohort_order, no_output=True, palette=palette)\n",
    "            plot_box_local(results_local_df, title=f\"{metric} local baseline\", metric=metric, no_output=True, palette=palette, hue_order=cohort_order)\n",
    "            plot_performance_multi_box_additive_cohort(results_comb_df, results_central_df, results_local_df, title=f\"{metric} comb analysis - additive cohorts\", metric=metric, no_output=True, palette=palette, hue_order=cohort_order)\n",
    "            plot_performance_line_additive_cohort(results_comb_df, results_central_df, results_local_df, title=f\"{metric} comb analysis - additive cohorts\", metric=metric, no_output=True, palette=palette, hue_order=cohort_order)\n",
    "            plot_performance_violin_additive_cohort(results_comb_df, results_central_df, results_local_df, title=f\"{metric} comb analysis - additive cohorts\", metric=metric, no_output=True, palette=palette, hue_order=cohort_order)\n",
    "            plot_performance_box_additive_cohort(results_comb_df, results_central_df, results_local_df, title=f\"{metric} comb analysis - additive cohorts\", metric=metric, no_output=True, palette=palette, hue_order=cohort_order)\n",
    "            plot_performance_box(results_comb_df, results_central_df, results_local_df, title=f\"{metric} comb analysis - by eval\", metric=metric, no_output=True, palette=palette, hue_order=cohort_order)\n",
    "            \n",
    "            plot_performance_linebox(results_comb_df, results_central_df, results_local_df, errorbar=None, title=f\"{metric} comb analysis - line by eval - no errorbar\", metric=metric, no_output=True, palette=palette, hue_order=cohort_order)\n",
    "            plot_performance_linebox(results_comb_df, results_central_df, results_local_df, errorbar='sd', title=f\"{metric} comb analysis - line by eval\", metric=metric, no_output=True, palette=palette, hue_order=cohort_order)\n",
    "            plot_performance_linebox(fedlike_results_comb_df, fedlike_results_central_df, fedlike_results_local_df, errorbar=None, title=f\"{metric} comb analysis - line by eval - fed datasets - no errorbar\", metric=metric, no_output=True, palette=palette, hue_order=cohort_order)\n",
    "            plot_performance_linebox(fedlike_results_comb_df, fedlike_results_central_df, fedlike_results_local_df, errorbar='sd', title=f\"{metric} comb analysis - line by eval - fed datasets\", metric=metric, no_output=True, palette=palette, hue_order=cohort_order)\n",
    "            \n",
    "            plot_performance_heatmap_additive_cohort(results_comb_df, results_local_df, show_core=False, metric=metric, title=None, no_output=True, max_clients=2, hue_order=cohort_order)\n",
    "            \n",
    "            for max_clients in range(1, min(results_comb_df['n_clients'].max() + 1, 12)):\n",
    "                plot_performance_heatmap(results_comb_df, results_local_df, show_core=False, metric=metric, no_output=True, max_clients=max_clients)\n",
    "            pass\n",
    " \n",
    "\n",
    "# Use ProcessPoolExecutor to run the plotting function concurrently for each experiment\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    futures = {executor.submit(plot_results_for_experiment, experiment, metrics, palette, cohort_order): experiment for experiment in experiments}\n",
    "    disable_telegram = False if is_server() else True\n",
    "    with telegram_tqdm(\n",
    "        total=len(futures), desc=f'plotting results', \n",
    "        token=telegram_token, chat_id=telegram_chatid, disable=disable_telegram\n",
    "    ) as pbar:\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            experiment = futures[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                pbar.update(1)\n",
    "            except Exception as exc:\n",
    "                print(f'{experiment.name} generated an exception: {exc}')\n",
    "                traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_summary(experiments):\n",
    "    summary_df = pd.DataFrame()\n",
    "    for experiment in experiments:\n",
    "        results_comb_df = load_results(experiment, \"combinations\")\n",
    "        summary_df = summarize_results(results_comb_df, summary_df, simulation_type='combinations')\n",
    "    summary_df = summary_df.reset_index(drop=True)\n",
    "    return summary_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with telegram_tqdm(metrics, total=len(metrics)+1, desc=\"Summary Metrics\", \n",
    "                   token=telegram_token, chat_id=telegram_chatid, disable=disable_telegram) as pbar:\n",
    "    pbar.set_description(f\"loading summary\")\n",
    "    # plot summary figures \n",
    "    summary_df = load_summary(experiments)\n",
    "    summary_core_df = split_data_by_train(summary_df, filter=['Brazil1', 'France1', 'USA1', 'USA2'])\n",
    "    summary_cohort_order = [experiment.name for experiment in experiments]\n",
    "    selected_cohort_order = [_ for _ in summary_cohort_order if \"func_\" not in _]\n",
    "    disable_telegram = False if is_server() else True\n",
    "    pbar.update(1)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for metric in metrics:\n",
    "            pbar.set_description(f\"plotting summary - {metric}\")\n",
    "            plot_selected_summary_performance(summary_df, metric=metric, title=f\"Dataset Configuration Performance Comparison - {metric}\", errorbar='sd', palette=palette, cohort_order=selected_cohort_order, save_dir=save_dir, no_output=True)\n",
    "            plot_selected_summary_performance(summary_df, metric=metric, title=f\"Dataset Configuration Performance Comparison - {metric} - no errorbars\", palette=palette, cohort_order=selected_cohort_order, save_dir=save_dir, no_output=True)\n",
    "            \n",
    "            plot_all_summary_performance(summary_df, metric=metric, title=f\"Dataset Configuration Performance Comparison - {metric} - no errorbars\", palette=palette, cohort_order=summary_cohort_order, save_dir=save_dir, no_output=True)\n",
    "            plot_all_summary_performance(summary_df, metric=metric, title=f\"Dataset Configuration Performance Comparison - {metric}\", errorbar=\"sd\", palette=palette, cohort_order=summary_cohort_order, save_dir=save_dir, no_output=True)\n",
    "            plot_all_summary_performance(summary_core_df, metric=metric, title=f\"Core Dataset Configuration Performance Comparison - {metric} - no errorbars\", palette=palette, cohort_order=summary_cohort_order, save_dir=save_dir, no_output=True)\n",
    "            plot_all_summary_performance(summary_core_df, metric=metric, title=f\"Core Dataset Configuration Performance Comparison - {metric}\", errorbar=\"sd\", palette=palette, cohort_order=summary_cohort_order, save_dir=save_dir, no_output=True)\n",
    "            pbar.update(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno['cohort_name'].value_counts(sort=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
